---
title: "Repaso PEP 1"
author: "Martin Salinas"
date: "2024-11-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Repaso practico para la PEP 1

## Para instalar paquetes en caso de no tenerlos

```{r}
if(!require(Nombre del paquete)){
  install.packages("nombre del paquete", dependencies=TRUE)
  require(nombre del paquete)
}

```

## Librerias usadas en la unidad:

```{r}
#Listado de todas las librerias utilizadas
```

## Manejo de datos

```{r}
#Data frame ejemplo

nombre <- c("Alan brito", "Zacarias", "Elsa payo")
prueba1 <- c(5.5, 3.4, 4.5)
prueba2 <- c(3.2, 4.3, 5.1)
prueba3 <- c(4.8, 4.3, 5.1)
fecha <- as.Date(c("2008-1-25" ,"2006-10-4","2008-3-27"))

dataframe <- data.frame(nombre,
                        fecha,
                        prueba1,
                        prueba2,
                        prueba3,
                        stringsAsFactors = FALSE)
```

Para trabajar los datos tenemos la biblioteca dplyr

```{r}
library(dplyr)


#Datos para ejemplo
datos <- iris

# Seleccionar observaciones correspondientes a la especie versicolor
versicolor <- datos %>% filter(Species == "versicolor")

# filtrar obs de la especie versicolor con petalos con longitud igual o  superior a 6 cm
largas <- datos %>% filter(Species == "versicolor" & Sepal.Length >= 6)

#Seleccionar la especie y variables relativas a los petalos
petalos <- datos %>% select(Species, starts_with("Petal"))

# Agregar a los datos de los petalos una nueva variable con la razon entre el largo y el ancho de estos
petalos <- petalos %>% mutate(Species, Petal.Width,
                              Petal.ratio = Petal.Length / Petal.Width)

# Ordenar el conjunto de datos de petalos en forma descedente segun la razo de los petalos
petalos <- petalos %>% arrange(desc(Petal.ratio))
```

Tambien se tiene la funcion para pivotear la matriz de datos pivot_longer(cols, names_to, values_to) del paquete tidyr

cols: nombres de las columnas a pivotar names_to: especifica el nombre de una nueva columna cuyos valores corresponden a los nombres de las columnas a pivotar values_to: especifica el nombre de una nueva columna donde se almacenan los valores de las columnas a pivotar

Para pivotear la matriza a formato ancho tenemos pivot_wider(names_from, values_from) de tidyr names_from: especifica el nombre de una variable desde la que se obtienen los nombres de las nuevas columnas values_from: especifica el nombre de una variable desde donde se obtienen los valoresd de las nuevas columnas

```{r}
library(dplyr)
library(tidyr)

instancia <- 1:6
Quicksort <- c(23.2, 22.6, 23.4, 23.3, 21.8, 23.9)
Bubblesort <- c(31.6, 29.3, 30.7, 30.8, 29.8,30.3)
Radixsort <- c(30.1, 28.4, 28.7, 28.3, 29.9, 29.1)
Mergesort <- c(1, 2, 3, 4, 5, 6)
datos <- data.frame(instancia, Quicksort, Bubblesort, Radixsort, Mergesort)

#Convertir matriz a formato a datos formato largo
datos_largos <- datos %>% pivot_longer(c("Quicksort","Bubblesort",
                                            "Radixsort","Mergesort"),
                                          names_to = "Algoritmo",
                                          values_to = "Tiempo")

datos_ancho <- datos_largos %>% pivot_wider(names_from = "Algoritmo",
                                            values_from = "Tiempo")

```

### Formulas

Las formulas siempre siguen el formato

<variable dependiente> \~ <variable independiente>

### Calculo de cuantiles

Para calcular cuantiles esta la funcion quantile()

Un ejemplo de como se utiliza seria:

```{r}
#Dado un conjunto de datos 
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

quantile(x)

```
0% valor minimo
25% Q1
50% Q2
75% Q3

## Lectura 2

Para obtener la desviacion estandar y la varianza aleatoria se tiene:

```{r variable discreta}
library(discreteRV)
library(ggpubr)

# Crear una variable discreta para representar el dado
# adulterado de la tabla 3.1.
resultados <- 1:6
probabilidades <- c(0.25, 0.125, 0.125, 0.125, 0.125, 0.25)
X <- RV(outcomes = resultados, probs = probabilidades)

# Calcular el valor esperado.
esperado <- E(X)
cat("Valor esperado:", esperado, "\n")

# Calcular la varianza.
varianza <- V(X)
cat("Varianza:", varianza, "\n")

# Calcular la desviación estándar.
desviacion <- SD(X)
cat("Desviación estándar:", desviacion, "\n")

#Crear vector con los resultados de 5 lanzamientos del dado
lanzar5 <- SofIID(X, n=5)

```

para continuas se tiene:

```{r}
library(ggpubr)

# Generar valores para una distribución normal con media 0 y
# desviación estándar 1.
media <- 0
desv_est <- 1
x <- seq(-15, 35, 0.01)
y <- dnorm(x, mean = media, sd = desv_est)
normal_1 <- data.frame(x, y)

# Repetir el proceso para una distribución normal con media 10
# y desviación estándar 6.
media <- 10
desv_est <- 6
x <- seq(-15, 35, 0.01)
y <- dnorm(x, mean = media, sd = desv_est)
normal_2 <- data.frame(x, y)

# Graficar ambas distribuciones.
g <- ggplot(normal_1, aes(x, y)) + geom_line(color = "blue")
g <- g + geom_line(data = normal_2, color = "red")
g <- g + theme_pubr()

print(g)

```

tenemos las funciones pnorm y qnorm que son inversas entre si, pnorm toma un vector cuantil, mientras que qnorm toma un vector de probabilidades

Para X^2 tenemos dchisq(x, df), pchisq(q, df, lower.tail), qchisq(p, df, lower.tail) y rchisq(n, df), df siendo los grados de libertad

Para t de student tenemos dt(q, df), pt(q, df, lower.tail), qt(p, df, lower.tail) y rt(n, df)

Para la distribucion F tenemos df(x, df1, df2), pf(q, df1, df2, lower.tail), qf(p, df1, df2, lower.tail) y rf(n, df1, df2)

### Distribuciones discretas

Para bernoulli tenemos dbern(x, prob), pbern(q, prob, lower.tail), qbern(p, prob, lower.tail) y rbern(n, prob)

Para la geometrica tenemos dgeom(x, prob), pgeom(q, prob, lower.tail), qgeom(p, prob, lower.tail) y rbern(n, prob)?

Para la binomial se tiene dbinom(x, size, prob), pbinom(q, size, prob), qbinom(p, size, prob) y rbinom(n, size, prob). size siendo el numero de intentos

Para la binomial negativa se tiene dnbinom(x, size, prob), pnbinom(q, size, prob, lower.tail), qnbinom(p, size, prob, lower.tail) y rnbinom(n, size, prob). x, qsiendo vector de cuantiles no negativos, size numero de exitos buscados, porb siendo probabilidad de exito

Para Poisson tenemos dpois(x, lambda), ppois(q, lambda, lower.tail), qqois(p, lambda, lower.tail) y rpois(n, lambda)

### Estimadores puntuales

```{r}
library(ggpubr)

# Establecer la semilla para generar números aleatorios.
set.seed(9437)

# Generar aleatoriamente una población de tamaño 1500
# (en este caso, con una distribución cercana a la normal).
poblacion <- rnorm(n = 1500, mean = 4.32, sd = 0.98)

# Calcular la media de la población.
media_poblacion <- mean(poblacion)
cat("Media de la población:", media_poblacion, "\n")

# Tomar una muestra de tamaño 1250.
tamano_muestra <- 1250
muestra <- sample(poblacion, tamano_muestra)

# Calcular las medias acumuladas (es decir, con muestras de 1, 2, 3, ... elementos).
n <- seq(along = muestra)
media <- cumsum(muestra) / n

# Crear una matriz de datos con los tamaños y las medias muestrales.
datos <- data.frame(n, media)

# Graficar las medias muestrales.
g <- ggline(data = datos,
            x = "n",
            y = "media",
            plot.type = "l",
            color = "blue",
            main = "Media móvil",
            xlab = "Tamaño de la muestra",
            ylab = "Media muestral")

# Añadir al gráfico una recta con la media de la población.
g <- g + geom_hline(aes(yintercept = media_poblacion),
                    color = "red", linetype = 2)

print(g)

```

## Lectura 3

### shapiro test

Funcion para saber si la muestra sigue una distribución normal

shapiro.test(x) x vector con las observaciones de la muestra.

### z test

Para la prueba z tenemos la funcion z.test(x, mu, stdev, alternative, conf.level)

x: vector con las observaciones de la muestra mu: valor nulo stdev: desviacion estandar de la población alternative: tipo de hipotesis alternativa, "two.sided","less","greater" conf.level: nivel de confianza

```{r}
library(TeachingDemos)
library(ggpubr)

muestra <- c(19, 29, 29.1, 32.1,25,0,22,31,36.9,31.4,17.6,23,20.6,24,28,26,29,29,26,27,20)

desv_est <- 2.32
n <- length(muestra)
valor_nulo <- 20

##Grafico qqplot
g <- ggqqplot(datos, x = "muestra", color = "steelBlue")
print(g)

#nivel de significación
alfa <- 0.01

media <- mean(muestra)

Z <- (media - valor_nulo) / (desv_est/ sqrt(n))

p <- 2 * pnorm(Z, lower.tail = FALSE)

prueba1 <- z.test(media, mu = valor_nulo, n = 20, alternative = "two.sided",stdev = desv_est, conf.level = 1-alfa)

prueba2 <- z.test(muestra, mu = valor_nulo, alternative = "two.sided",stdev = desv_est, conf.level = 1-alfa)

```

### Prueba t de student

```{r prueba t de student}
library(ggpubr)

tiempo <- c(411, 393, 445 ,498, 231, 134 ,445,430, 408,  463, 409,418)

alfa <- 0.025
valor_nulo <- 500

t.test(tiempo, 
       alternative = "less",
       mu = valor_nulo,
       conf.level = 1 - alfa)

#Sean 2 muestras t_A y t_B

diferencia <- t_A - t_B

prueba1 <- t.test(diferencia,
                  alternative = "two.sided",
                  mu = valor_nulo,
                  conf.level = 1 - alfa)

prueba2 <- t.test(x = t_A,
                  y = t_B,
                  paired = TRUE,
                  alternative = "two.sided",
                  mu = valor_nulo,
                  conf.leve = 1 - alfa)

#Para independientes lo unico que cambia es el paired que cambia de FALSE a TRUE y que solo se puede hacer como la prueba 2

```

### Metodo de Wald

```{r Metodo de Wald}
#Valores
n <- 150
p_exito <- 0.64
alfa <- 0.05
valor_nulo <- 0.7

#Construccion intervalo de confianza
error_est <- sqrt((p_exito * (1 - p_exito))/n)
Z_critico <- qnorm(alfa / 2, lower.tail = FALSE)
inferior <- p_exito - Z_critico * error_est
superior <- p_exito + Z_critico * error_est

# Prueba de hipotesis
error_est_hip <- sqrt((valor_nulo * (1 - valor_nulo)) / n)
z <- (p_exito - valor_nulo) / error_est_hip
p <- pnorm(Z, lower.tail = FALSE)

```

```{r wald para dos proporciones}
n_hombres <- 48
n_mujeres <- 42
exitos_hombres <- 26
exitos_mujeres <- 20
alfa <- 0.05
valor_nulo <- 0

# Calcular probabilidades de éxito.
p_hombres <- exitos_hombres / n_hombres
p_mujeres <- exitos_mujeres / n_mujeres

# Estimar la diferencia.
diferencia <- p_hombres - p_mujeres

# Construcción del intervalo de confianza.
error_hombres <- (p_hombres * (1 - p_hombres)) / n_hombres
error_mujeres <- (p_mujeres * (1 - p_mujeres)) / n_mujeres
error_est <- sqrt(error_hombres + error_mujeres)
z_critico <- qnorm(alfa / 2, lower.tail = FALSE)
inferior <- diferencia - z_critico * error_est
superior <- diferencia + z_critico * error_est
cat("Intervalo de confianza = [", inferior, ", ", superior, "]\n", sep = "")

# Prueba de hipótesis.
p_agrupada <- (exitos_hombres + exitos_mujeres) / (n_hombres + n_mujeres)
error_hombres <- (p_agrupada * (1 - p_agrupada)) / n_hombres
error_mujeres <- (p_agrupada * (1 - p_agrupada)) / n_mujeres
error_est_hip <- sqrt(error_hombres + error_mujeres)
z <- (diferencia - valor_nulo) / error_est_hip
p_valor <- 2 * pnorm(z, lower.tail = FALSE)


```

### Metodo de wilson

```{r metodo de wilson}
# Script 6.4: método de Wilson para una proporción.

# Fijar valores conocidos
n <- 150
p_exito <- 0.64
alfa <- 0.05
valor_nulo <- 0.7

# Calcular cantidad de éxitos.
exitos <- p_exito * n

# Prueba de Wilson en R.
prueba <- prop.test(exitos, n = n, p = valor_nulo,
                    alternative = "greater", conf.level = 1 - alfa)
print(prueba)
```

```{r método de Wilson para la diferencia entre dos proporciones}
# Fijar valores conocidos (hombres, mujeres)
n <- c(48, 42)
exitos <- c(26, 20)
alfa <- 0.05

# Prueba de Wilson en R.
prueba <- prop.test(exitos, n = n, alternative = "two.sided",
                    conf.level = 1 - alfa)
print(prueba)
```

